# -*- coding: utf-8 -*-
"""APS360Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UmxcUgQpj01IulVcXbKIdXpbpYWC7awN
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html "/content/APS360Project.ipynb"

import os
import random
import numpy as np
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.io as torchio
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision.transforms as transforms
import torchvision.datasets as tds
import matplotlib.pyplot as plt
import pandas as pd
import PIL

# Mounting Google Drive
#from google.colab import drive
#drive.mount('/content/drive')

def get_model_name(name, batch_size, learning_rate, epoch):
    """ Generate a name for the model consisting of all the hyperparameter values

    Args:
        config: Configuration object containing the hyperparameters
    Returns:
        path: A string with the hyperparameter name and value concatenated
    """
    path = "model_{0}_bs{1}_lr{2}_epoch{3}".format(name,batch_size,learning_rate,epoch)
    return path

# Save the current model (checkpoint) to a file
# model_path = get_model_name(xxx.name, batch_size, learning_rate, epoch)
# torch.save(xxx.state_dict(), model_path)
#--------------------------------------------------------------------------------------

class Autoencoder(nn.Module):
    # 32 is what the paper starts with
    def __init__(self, startOutCh = 32, depthRatio = 16 / 9):
        super(Autoencoder, self).__init__()

        # Added Name
        self.name = "AEhalf_sod{0}_odr{1:.2f}".format(startOutCh, depthRatio)
        self.startOutCh = startOutCh

        startOutCh2 = int(startOutCh * depthRatio)
        startOutCh3 = int(startOutCh2 * depthRatio)

        # half the # of convolutional blocks from the paper
        self.model = nn.Sequential(
            # encoder
            nn.Conv2d(in_channels=3, out_channels=startOutCh, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(4, 4),
            nn.Conv2d(startOutCh, startOutCh2, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(4, 4),
            nn.Conv2d(startOutCh2, startOutCh3, 3, 1, 1),

            # decoder
            nn.ConvTranspose2d(startOutCh3, startOutCh2, 3, 1, 1),
            nn.ReLU(),
            nn.UpsamplingNearest2d(scale_factor=4),
            nn.ConvTranspose2d(startOutCh2, startOutCh, 3, 1, 1),
            nn.ReLU(),
            nn.UpsamplingNearest2d(scale_factor=4),
            nn.ConvTranspose2d(startOutCh, 3, 3, 1, 1)
        )

    def forward(self, x):
        return self.model(x)


def train(model, model_dir, train_ds, valid_ds, num_epochs=5, batch_size=1, lr=1e-3):
    torch.manual_seed(42)
    criterion = nn.MSELoss() # mean square error loss
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)

    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)

    #outputs = []
    for epoch in range(num_epochs):
        for data in train_loader:
            noisy, truth = data
            recon = model(noisy)
            loss = criterion(recon, truth)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))
        plt.imshow(np.transpose(truth.detach().numpy()[0], (1, 2, 0)))
        plt.show()
        plt.imshow(np.transpose(recon.detach().numpy()[0], (1, 2, 0)))
        plt.show()

        # Save the current model (checkpoint) to a file
        model_path = get_model_name(model.name, batch_size, lr, epoch + 1)
        torch.save(model.state_dict(), os.path.join(model_dir, model_path))

        #outputs.append((epoch, img, recon),)
    #return outputs
    return truth, recon

#plt.imshow(np.transpose(final_recon[0][0].detach().numpy()), (1, 2, 0))

# Address for Datasets within the Drive
#basePath = "/content/drive/MyDrive/"
basePath = ""
truthPaths = [basePath + "data/baseline_data/animations/anim1/4096/",
              basePath + "data/baseline_data/animations/anim2/4096/"]
noisyPaths = [basePath + "data/baseline_data/animations/anim1/1/",
              basePath + "data/baseline_data/animations/anim2/1/"]

def get_io_paths(noisy_dirs, truth_dirs):
  """
        noisy_dirs: List of noisy image directories.
        truth_dirs: List of truth image directories.
  """
  assert len(noisy_dirs) == len(truth_dirs)

  paths = []
  for i in range(len(noisy_dirs)):
    nfiles = [os.path.join(noisy_dirs[i], f) for f in os.listdir(noisy_dirs[i])]
    tfiles = [os.path.join(truth_dirs[i], f) for f in os.listdir(truth_dirs[i])]
    nfiles.sort()
    tfiles.sort()
    assert len(nfiles) == len(tfiles)
    paths.extend(zip(nfiles, tfiles))

  return paths

# this is the only thing that works to avoid running out of RAM in Colab
class ImagesDataset(torch.utils.data.Dataset):
    def __init__(self, io_paths, transform=None):
        """
        io_paths: list of tuples of input-output image paths.
        transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.paths = io_paths
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
      noisy_img = torchio.read_image(self.paths[idx][0], torchio.ImageReadMode.RGB).to(torch.float)
      truth_img = torchio.read_image(self.paths[idx][1], torchio.ImageReadMode.RGB).to(torch.float)
      noisy_img /= 255.
      truth_img /= 255.

      if (self.transform):
        noisy_img = self.transform(noisy_img)
        truth_img = self.transform(truth_img)

      return [noisy_img, truth_img]

train_split = 0.7
valid_split = 0.15
test_split = 0.15

paths = get_io_paths(noisyPaths, truthPaths)
random.shuffle(paths)
train_eidx = int(len(paths) * train_split)
valid_eidx = int(len(paths) * (train_split + valid_split))

trans128 = transforms.CenterCrop(size=128)
train_ds = ImagesDataset(paths[:train_eidx], transform=trans128)
valid_ds = ImagesDataset(paths[train_eidx:valid_eidx], transform=trans128)
test_ds = ImagesDataset(paths[valid_eidx:], transform=trans128)


model = Autoencoder(depthRatio=16/3)
model_dir = basePath + "data/nnmodel/{}".format(model.name)
if not os.path.exists(model_dir):
  os.mkdir(model_dir)

fpaths = open(os.path.join(model_dir, "paths.txt"), "w")
fpaths.write("split={0},{1},{2}".format(train_split, valid_split, test_split) + "\n")
for p in paths:
  fpaths.write(str(p) + "\n")
fpaths.close()

final_truth, final_recon = train(model, model_dir, train_ds, valid_ds, batch_size=32, num_epochs=50, lr=1e-3)

#train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2)
#plt.imshow(np.transpose(np.array(next(iter(train_loader))[0][0]), (1, 2, 0)))

plt.imshow(np.transpose(final_recon.detach().numpy()[0], (1, 2, 0)))