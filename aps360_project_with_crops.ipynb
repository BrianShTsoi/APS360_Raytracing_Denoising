{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNgr3Hl5FJOa",
        "outputId": "e4ccfa6a-1e74-4f31-e051-8de0f2e03257"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%shell` not found.\n"
          ]
        }
      ],
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html \"/content/APS360Project.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2kTRL7K9DfXZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.io as torchio\n",
        "from   torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.datasets as tds\n",
        "import matplotlib.pyplot as plt\n",
        "#import pandas as pd\n",
        "import PIL\n",
        "import shutil\n",
        "\n",
        "# Mounting Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "LOCAL = True\n",
        "SHOW_RESULTS = False # !LOCAL\n",
        "# set to true when animation images have changed to rebuild\n",
        "# all the 128x128 crops. After first run, this can be set back to false\n",
        "REFRESH_CROPS = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1uRn4h2bdrUF"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    # 32 is what the paper starts with\n",
        "    def __init__(self, startOutCh = 32, depthRatio = 16 / 9):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Added Name\n",
        "        self.name = \"AEhalfmdatar3_sod{0}_odr{1:.2f}\".format(startOutCh, depthRatio)\n",
        "        self.startOutCh = startOutCh\n",
        "        \n",
        "        # Values\n",
        "        startOutCh2 = int(startOutCh * depthRatio)\n",
        "        startOutCh3 = int(startOutCh2 * depthRatio)\n",
        "        startOutCh4 = int(startOutCh3 * depthRatio)\n",
        "        \n",
        "        # Convolution Layers\n",
        "        self.Conv2D_1 = nn.Conv2d(in_channels = 3, out_channels = startOutCh, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.Conv2D_2 = nn.Conv2d(startOutCh, startOutCh2, 3, 1, 1)\n",
        "        self.Conv2D_3 = nn.Conv2d(startOutCh2, startOutCh3, 3, 1, 1)\n",
        "        self.Conv2D_4 = nn.Conv2d(startOutCh3, startOutCh4, 3, 1, 1)\n",
        "        \n",
        "        self.Conv2D_T1 = nn.ConvTranspose2d(startOutCh4, startOutCh3, 3, 1, 1)\n",
        "        self.Conv2D_T2 = nn.ConvTranspose2d(startOutCh3, startOutCh2, 3, 1, 1)\n",
        "        self.Conv2D_T3 = nn.ConvTranspose2d(startOutCh2, startOutCh, 3, 1, 1)\n",
        "        self.Conv2D_T4 = nn.ConvTranspose2d(startOutCh, 3, 3, 1, 1)\n",
        "        \n",
        "        # Pooling & Up-Scaling Layers\n",
        "        self.Pooling_1 = nn.MaxPool2d(4, 4)\n",
        "        self.Expanding_1 = nn.UpsamplingNearest2d(scale_factor = 4)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.Sigmoid = nn.Sigmoid()      \n",
        "\n",
        "    def forward(self, x):   \n",
        "        x1 = self.Conv2D_1(x)\n",
        "        x = self.ReLU(self.Pooling_1(x1))\n",
        "        x2 = self.Conv2D_2(x)\n",
        "        x = self.ReLU(self.Pooling_1(x2))\n",
        "        x3 = self.Conv2D_3(x)\n",
        "        x = self.ReLU(self.Pooling_1(x3))\n",
        "        \n",
        "        x4 = self.Conv2D_4(x)\n",
        "        # x = self.ReLU(x4)\n",
        "        x = self.Conv2D_T1(x4)\n",
        "        x = self.ReLU(x) + self.Conv2D_T1(x4)\n",
        "        \n",
        "        x = self.Expanding_1(self.Conv2D_T2(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T2(x3)\n",
        "        x = self.Expanding_1(self.Conv2D_T3(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T3(x2)\n",
        "        x = self.Expanding_1(self.Conv2D_T4(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T4(x1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qHSsWBBmdrUF",
        "outputId": "93e5a7f8-6698-43b0-f215-db930385d303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Aug 10 08:55:48 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 536.99                 Driver Version: 536.99       CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   57C    P5              11W /  80W |    267MiB /  4096MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A     16724    C+G   ... Foundation\\Blender 3.6\\blender.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "model = Autoencoder()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pjLbfnzIdrUG",
        "outputId": "dc789072-ee09-4a52-edb5-66727b499d53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1795584"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f__SythpGar1"
      },
      "outputs": [],
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch=None):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\"\"\"\n",
        "    if epoch is None:\n",
        "        return \"model_{0}_bs{1}_lr{2}\".format(name, batch_size, learning_rate)\n",
        "    else:\n",
        "        return \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, learning_rate, epoch)\n",
        "\n",
        "def plt_img_tensor(tensor):\n",
        "    t = torch.transpose(torch.transpose(tensor, 0, 2), 0, 1)\n",
        "    plt.imshow(t.detach().cpu())\n",
        "    plt.show()\n",
        "\n",
        "def save_img_tensor(tensor, save_dir, img_name):\n",
        "    t = torch.clamp(tensor * 255., min=0., max=255.).byte().detach().cpu()\n",
        "    torchio.write_png(t, save_dir + img_name + \".png\", compression_level=0)\n",
        "\n",
        "\n",
        "def train(model, model_dir, result_dir, train_ds, valid_ds, num_epochs=5, batch_size=1, lr=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.L1Loss() # L1 Loss is used for model updates\n",
        "    criterion_compair = nn.MSELoss() # mean square error loss for standarization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            \n",
        "            noisy, truth = data            \n",
        "            recon = model(noisy)\n",
        "            loss = criterion(recon, truth)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        loss_oimg = criterion_compair(recon, truth)\n",
        "        print('Epoch:{}, MSE Loss:{:.4f}'.format(epoch+1, float(loss_oimg)))\n",
        "\n",
        "        for i in range(recon.size()[0]):\n",
        "            save_img_tensor(truth[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_truth\")\n",
        "            save_img_tensor(recon[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_recon\")\n",
        "            save_img_tensor(noisy[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_noisy\")\n",
        "\n",
        "        if SHOW_RESULTS:\n",
        "            plt_img_tensor(truth[0])\n",
        "            plt_img_tensor(recon[0])\n",
        "\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(model.name, batch_size, lr, epoch + 1)\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, model_path))\n",
        "\n",
        "        #outputs.append((epoch, img, recon),)\n",
        "    #return outputs\n",
        "\n",
        "#plt.imshow(np.transpose(final_recon[0][0].detach().numpy()), (1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BhBZrf-ek90l"
      },
      "outputs": [],
      "source": [
        "# Address for Datasets within the Drive\n",
        "BasePath = \"data/\" if LOCAL else \"/content/drive/MyDrive/data/\"\n",
        "BaseAnimPath = BasePath + \"baseline_data/animations/\"\n",
        "truthPaths = [BaseAnimPath + \"anim1/4096\",\n",
        "              BaseAnimPath + \"anim2/4096\",\n",
        "              BaseAnimPath + \"anim3/4096\",\n",
        "              BaseAnimPath + \"anim5/4096\"]\n",
        "noisyPaths = [BaseAnimPath + \"anim1/1\",\n",
        "              BaseAnimPath + \"anim2/1\",\n",
        "              BaseAnimPath + \"anim3/1\",\n",
        "              BaseAnimPath + \"anim5/1\"]\n",
        "\n",
        "def get_io_paths(noisy_dirs, truth_dirs):\n",
        "  \"\"\"\n",
        "        noisy_dirs: List of noisy image directories.\n",
        "        truth_dirs: List of truth image directories.\n",
        "  \"\"\"\n",
        "  assert len(noisy_dirs) == len(truth_dirs)\n",
        "\n",
        "  paths = []\n",
        "  for i in range(len(noisy_dirs)):\n",
        "    nfiles = [os.path.join(noisy_dirs[i], f) for f in os.listdir(noisy_dirs[i])]\n",
        "    tfiles = [os.path.join(truth_dirs[i], f) for f in os.listdir(truth_dirs[i])]\n",
        "    nfiles.sort()\n",
        "    tfiles.sort()\n",
        "    # assert len(nfiles) == len(tfiles)\n",
        "    paths.extend(zip(nfiles, tfiles))\n",
        "\n",
        "  return paths\n",
        "\n",
        "# this is the only thing that works to avoid running out of RAM in Colab\n",
        "class ImagesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, io_paths, transform=None):\n",
        "        \"\"\"\n",
        "        io_paths: list of tuples of input-output image paths.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.paths = io_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      noisy_img = torchio.read_image(self.paths[idx][0], torchio.ImageReadMode.RGB).to(torch.float)\n",
        "      truth_img = torchio.read_image(self.paths[idx][1], torchio.ImageReadMode.RGB).to(torch.float)\n",
        "\n",
        "      if use_cuda and torch.cuda.is_available():\n",
        "        noisy_img = noisy_img.cuda()\n",
        "        truth_img = truth_img.cuda()\n",
        "         \n",
        "      noisy_img /= 255.\n",
        "      truth_img /= 255.\n",
        "\n",
        "      if (self.transform):\n",
        "        noisy_img = self.transform(noisy_img)\n",
        "        truth_img = self.transform(truth_img)\n",
        "\n",
        "      return [noisy_img, truth_img]\n",
        "\n",
        "\n",
        "def write_crops(img, img_idx, dir):\n",
        "  crop_paths = []\n",
        "  for j in range(64):\n",
        "    crop_idx_x = j % 8\n",
        "    crop_idx_y = int(j / 8)\n",
        "    img_crop = TF.crop(img, 128 * crop_idx_y, 128 * crop_idx_x, 128, 128)\n",
        "    crop_path = dir + str(img_idx) + \"_\" + str(j) + \".png\"\n",
        "    torchio.write_png(img_crop, crop_path, compression_level=0)\n",
        "    crop_paths.append(crop_path)\n",
        "  return crop_paths\n",
        "\n",
        "\n",
        "train_split = 0.7\n",
        "valid_split = 0.15\n",
        "test_split = 0.15\n",
        "\n",
        "crop_dir = BaseAnimPath + \"crops/\"\n",
        "noisy_crop_dir = crop_dir + \"noisy/\"\n",
        "truth_crop_dir = crop_dir + \"truth/\"\n",
        "\n",
        "if REFRESH_CROPS and os.path.exists(crop_dir):\n",
        "  shutil.rmtree(crop_dir)\n",
        "\n",
        "if os.path.exists(crop_dir):\n",
        "  paths = get_io_paths([noisy_crop_dir], [truth_crop_dir])\n",
        "else:\n",
        "  paths = get_io_paths(noisyPaths, truthPaths)\n",
        "  new_paths = []\n",
        "  os.mkdir(crop_dir)\n",
        "  os.mkdir(noisy_crop_dir)\n",
        "  os.mkdir(truth_crop_dir)\n",
        "\n",
        "  for i in range(len(paths)):\n",
        "    noisy_img = torchio.read_image(paths[i][0], torchio.ImageReadMode.RGB)\n",
        "    truth_img = torchio.read_image(paths[i][1], torchio.ImageReadMode.RGB)\n",
        "    ncrop_paths = write_crops(noisy_img, i, noisy_crop_dir)\n",
        "    tcrop_paths = write_crops(truth_img, i, truth_crop_dir)\n",
        "    new_paths.extend(zip(ncrop_paths, tcrop_paths))\n",
        "\n",
        "  paths = new_paths\n",
        "  \n",
        "random.shuffle(paths)\n",
        "train_eidx = int(len(paths) * train_split)\n",
        "valid_eidx = int(len(paths) * (train_split + valid_split))\n",
        "\n",
        "#trans256 = transforms.CenterCrop(size = 256)\n",
        "train_ds = ImagesDataset(paths[:train_eidx])#, transform=trans256)\n",
        "valid_ds = ImagesDataset(paths[train_eidx:valid_eidx])#, transform=trans256)\n",
        "test_ds = ImagesDataset(paths[valid_eidx:])#, transform=trans256)\n",
        "\n",
        "BatchSize = 16\n",
        "LearningRate = 5e-4\n",
        "\n",
        "\n",
        "model = Autoencoder().cuda()\n",
        "model_dir = BasePath + \"nnmodel/{}/\".format(model.name)\n",
        "if not os.path.exists(model_dir):\n",
        "  os.mkdir(model_dir)\n",
        "  \n",
        "result_dir = model_dir + get_model_name(model.name, BatchSize, LearningRate) + \"_results/\"\n",
        "if not os.path.exists(result_dir):\n",
        "    os.mkdir(result_dir)\n",
        "\n",
        "fpaths = open(os.path.join(model_dir, \"paths.txt\"), \"w\")\n",
        "fpaths.write(\"split={0},{1},{2}\".format(train_split, valid_split, test_split) + \"\\n\")\n",
        "for p in paths:\n",
        "  fpaths.write(str(p) + \"\\n\")\n",
        "fpaths.close()\n",
        "\n",
        "#train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2)\n",
        "#plt.imshow(np.transpose(np.array(next(iter(train_loader))[0][0]), (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"data/nnmodel/AEhalfmdatar3_sod32_odr1.78/model_AEhalfmdatar3_sod32_odr1.78_bs16_lr0.0005_epoch75\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_iJJXH1mdrUH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 1024, 1024])\n",
            "MSE loss recon 0.012149984948337078\n",
            "MSE loss guassian blur k5 0.0750330463051796\n"
          ]
        }
      ],
      "source": [
        "demo_path = r\"E:\\Maya\\UofT\\8th sem\\aps360\\project\\APS360_raytracing_denoising\\localver\\test2\\breakfast32.png\"\n",
        "gt_path = r\"E:\\Maya\\UofT\\8th sem\\aps360\\project\\APS360_raytracing_denoising\\localver\\test2\\breakfast8192.png\"\n",
        "gblur_path = r\"E:\\Maya\\UofT\\8th sem\\aps360\\project\\APS360_raytracing_denoising\\localver\\test2\\breakfast32_guassianblur.png\"\n",
        "\n",
        "demo_ds = ImagesDataset([(demo_path, gt_path), (gblur_path, gt_path),])\n",
        "#save_img_tensor(next(iter(demo_ds))[0], \"E:\\\\Maya\\\\UofT\\\\8th sem\\\\aps360\\\\project\\\\APS360_raytracing_denoising\\\\localver\\\\\", \"demo1cropped.png\")\n",
        "\n",
        "demo_dl = torch.utils.data.DataLoader(demo_ds, batch_size=1)\n",
        "\n",
        "demo_dat = next(iter(demo_dl))\n",
        "demo_noisy, demo_gt = demo_dat\n",
        "demo_dat2 = next(iter(demo_dl))\n",
        "demo_gblur, _ = demo_dat2\n",
        "\n",
        "demo_recon = model(demo_noisy)\n",
        "\n",
        "cmp = nn.MSELoss()\n",
        "print(\"MSE loss recon\", float(cmp(demo_recon, demo_gt)))\n",
        "print(\"MSE loss guassian blur k5\", float(cmp(demo_gblur, demo_gt)))\n",
        "\n",
        "save_img_tensor(demo_recon[0], \"E:\\\\Maya\\\\UofT\\\\8th sem\\\\aps360\\\\project\\\\APS360_raytracing_denoising\\\\localver\\\\test2\\\\\", \"breakfast_recon\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oiQICbM8drUH",
        "outputId": "b17a47a5-5f37-44a0-d868-838fb3dab0c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, MSE Loss:0.0003\n",
            "Epoch:2, MSE Loss:0.0008\n",
            "Epoch:3, MSE Loss:0.0008\n",
            "Epoch:4, MSE Loss:0.0014\n",
            "Epoch:5, MSE Loss:0.0004\n",
            "Epoch:6, MSE Loss:0.0004\n",
            "Epoch:7, MSE Loss:0.0022\n",
            "Epoch:8, MSE Loss:0.0005\n",
            "Epoch:9, MSE Loss:0.0007\n",
            "Epoch:10, MSE Loss:0.0012\n",
            "Epoch:11, MSE Loss:0.0008\n",
            "Epoch:12, MSE Loss:0.0019\n",
            "Epoch:13, MSE Loss:0.0005\n",
            "Epoch:14, MSE Loss:0.0010\n",
            "Epoch:15, MSE Loss:0.0011\n",
            "Epoch:16, MSE Loss:0.0010\n",
            "Epoch:17, MSE Loss:0.0008\n",
            "Epoch:18, MSE Loss:0.0008\n",
            "Epoch:19, MSE Loss:0.0013\n",
            "Epoch:20, MSE Loss:0.0010\n",
            "Epoch:21, MSE Loss:0.0009\n",
            "Epoch:22, MSE Loss:0.0006\n",
            "Epoch:23, MSE Loss:0.0005\n",
            "Epoch:24, MSE Loss:0.0027\n",
            "Epoch:25, MSE Loss:0.0013\n",
            "Epoch:26, MSE Loss:0.0018\n",
            "Epoch:27, MSE Loss:0.0015\n",
            "Epoch:28, MSE Loss:0.0021\n",
            "Epoch:29, MSE Loss:0.0006\n",
            "Epoch:30, MSE Loss:0.0027\n",
            "Epoch:31, MSE Loss:0.0009\n",
            "Epoch:32, MSE Loss:0.0003\n",
            "Epoch:33, MSE Loss:0.0005\n",
            "Epoch:34, MSE Loss:0.0006\n",
            "Epoch:35, MSE Loss:0.0008\n",
            "Epoch:36, MSE Loss:0.0006\n",
            "Epoch:37, MSE Loss:0.0014\n",
            "Epoch:38, MSE Loss:0.0009\n",
            "Epoch:39, MSE Loss:0.0009\n",
            "Epoch:40, MSE Loss:0.0008\n",
            "Epoch:41, MSE Loss:0.0007\n",
            "Epoch:42, MSE Loss:0.0017\n",
            "Epoch:43, MSE Loss:0.0016\n",
            "Epoch:44, MSE Loss:0.0022\n",
            "Epoch:45, MSE Loss:0.0007\n",
            "Epoch:46, MSE Loss:0.0010\n",
            "Epoch:47, MSE Loss:0.0004\n",
            "Epoch:48, MSE Loss:0.0011\n",
            "Epoch:49, MSE Loss:0.0025\n",
            "Epoch:50, MSE Loss:0.0006\n",
            "Epoch:51, MSE Loss:0.0014\n",
            "Epoch:52, MSE Loss:0.0005\n",
            "Epoch:53, MSE Loss:0.0007\n",
            "Epoch:54, MSE Loss:0.0022\n",
            "Epoch:55, MSE Loss:0.0005\n",
            "Epoch:56, MSE Loss:0.0006\n",
            "Epoch:57, MSE Loss:0.0009\n",
            "Epoch:58, MSE Loss:0.0008\n",
            "Epoch:59, MSE Loss:0.0017\n",
            "Epoch:60, MSE Loss:0.0009\n",
            "Epoch:61, MSE Loss:0.0004\n",
            "Epoch:62, MSE Loss:0.0005\n",
            "Epoch:63, MSE Loss:0.0020\n",
            "Epoch:64, MSE Loss:0.0015\n",
            "Epoch:65, MSE Loss:0.0009\n",
            "Epoch:66, MSE Loss:0.0007\n",
            "Epoch:67, MSE Loss:0.0012\n",
            "Epoch:68, MSE Loss:0.0003\n",
            "Epoch:69, MSE Loss:0.0012\n",
            "Epoch:70, MSE Loss:0.0018\n",
            "Epoch:71, MSE Loss:0.0005\n",
            "Epoch:72, MSE Loss:0.0007\n",
            "Epoch:73, MSE Loss:0.0019\n",
            "Epoch:74, MSE Loss:0.0004\n",
            "Epoch:75, MSE Loss:0.0009\n",
            "Epoch:76, MSE Loss:0.0007\n",
            "Epoch:77, MSE Loss:0.0008\n",
            "Epoch:78, MSE Loss:0.0009\n",
            "Epoch:79, MSE Loss:0.0006\n",
            "Epoch:80, MSE Loss:0.0013\n",
            "Epoch:81, MSE Loss:0.0002\n",
            "Epoch:82, MSE Loss:0.0004\n",
            "Epoch:83, MSE Loss:0.0008\n",
            "Epoch:84, MSE Loss:0.0005\n",
            "Epoch:85, MSE Loss:0.0015\n",
            "Epoch:86, MSE Loss:0.0018\n",
            "Epoch:87, MSE Loss:0.0015\n",
            "Epoch:88, MSE Loss:0.0005\n",
            "Epoch:89, MSE Loss:0.0017\n",
            "Epoch:90, MSE Loss:0.0006\n",
            "Epoch:91, MSE Loss:0.0018\n",
            "Epoch:92, MSE Loss:0.0007\n",
            "Epoch:93, MSE Loss:0.0022\n",
            "Epoch:94, MSE Loss:0.0006\n",
            "Epoch:95, MSE Loss:0.0004\n",
            "Epoch:96, MSE Loss:0.0018\n",
            "Epoch:97, MSE Loss:0.0010\n",
            "Epoch:98, MSE Loss:0.0006\n",
            "Epoch:99, MSE Loss:0.0006\n",
            "Epoch:100, MSE Loss:0.0017\n",
            "Epoch:101, MSE Loss:0.0009\n",
            "Epoch:102, MSE Loss:0.0007\n",
            "Epoch:103, MSE Loss:0.0004\n",
            "Epoch:104, MSE Loss:0.0018\n",
            "Epoch:105, MSE Loss:0.0004\n",
            "Epoch:106, MSE Loss:0.0007\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, model_dir, result_dir, train_ds, valid_ds, batch_size \u001b[39m=\u001b[39;49m BatchSize, num_epochs \u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m, lr \u001b[39m=\u001b[39;49m LearningRate)\n",
            "Cell \u001b[1;32mIn[39], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, model_dir, result_dir, train_ds, valid_ds, num_epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m#outputs = []\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 28\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     30\u001b[0m         noisy, truth \u001b[39m=\u001b[39m data            \n\u001b[0;32m     31\u001b[0m         recon \u001b[39m=\u001b[39m model(noisy)\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[40], line 46\u001b[0m, in \u001b[0;36mImagesDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     45\u001b[0m   noisy_img \u001b[39m=\u001b[39m torchio\u001b[39m.\u001b[39mread_image(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpaths[idx][\u001b[39m0\u001b[39m], torchio\u001b[39m.\u001b[39mImageReadMode\u001b[39m.\u001b[39mRGB)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m---> 46\u001b[0m   truth_img \u001b[39m=\u001b[39m torchio\u001b[39m.\u001b[39;49mread_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpaths[idx][\u001b[39m1\u001b[39;49m], torchio\u001b[39m.\u001b[39;49mImageReadMode\u001b[39m.\u001b[39;49mRGB)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m     48\u001b[0m   \u001b[39mif\u001b[39;00m use_cuda \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m     49\u001b[0m     noisy_img \u001b[39m=\u001b[39m noisy_img\u001b[39m.\u001b[39mcuda()\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\image.py:259\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m    258\u001b[0m data \u001b[39m=\u001b[39m read_file(path)\n\u001b[1;32m--> 259\u001b[0m \u001b[39mreturn\u001b[39;00m decode_image(data, mode)\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\image.py:236\u001b[0m, in \u001b[0;36mdecode_image\u001b[1;34m(input, mode)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m    235\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[1;32m--> 236\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mdecode_image(\u001b[39minput\u001b[39;49m, mode\u001b[39m.\u001b[39;49mvalue)\n\u001b[0;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(model, model_dir, result_dir, train_ds, valid_ds, batch_size = BatchSize, num_epochs = 500, lr = LearningRate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "544c7e72ceabb8b5baf00146d00b7c3a5bfd60d2cf41bb549c5c035fb33480da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
