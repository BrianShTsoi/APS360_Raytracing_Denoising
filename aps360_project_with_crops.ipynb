{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNgr3Hl5FJOa",
        "outputId": "e4ccfa6a-1e74-4f31-e051-8de0f2e03257"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%shell` not found.\n"
          ]
        }
      ],
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html \"/content/APS360Project.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2kTRL7K9DfXZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.io as torchio\n",
        "from   torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.datasets as tds\n",
        "import matplotlib.pyplot as plt\n",
        "#import pandas as pd\n",
        "import PIL\n",
        "import shutil\n",
        "\n",
        "# Mounting Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "LOCAL = True\n",
        "# set to true when animation images have changed to rebuild\n",
        "# all the 128x128 crops. After first run, this can be set back to false\n",
        "REFRESH_CROPS = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1uRn4h2bdrUF"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    # 32 is what the paper starts with\n",
        "    def __init__(self, startOutCh = 32, depthRatio = 16 / 9):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Added Name\n",
        "        self.name = \"AEhalfmdata_sod{0}_odr{1:.2f}\".format(startOutCh, depthRatio)\n",
        "        self.startOutCh = startOutCh\n",
        "        \n",
        "        # Values\n",
        "        startOutCh2 = int(startOutCh * depthRatio)\n",
        "        startOutCh3 = int(startOutCh2 * depthRatio)\n",
        "        startOutCh4 = int(startOutCh3 * depthRatio)\n",
        "        \n",
        "        # Convolution Layers\n",
        "        self.Conv2D_1 = nn.Conv2d(in_channels = 3, out_channels = startOutCh, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.Conv2D_2 = nn.Conv2d(startOutCh, startOutCh2, 3, 1, 1)\n",
        "        self.Conv2D_3 = nn.Conv2d(startOutCh2, startOutCh3, 3, 1, 1)\n",
        "        self.Conv2D_4 = nn.Conv2d(startOutCh3, startOutCh4, 3, 1, 1)\n",
        "        \n",
        "        self.Conv2D_T1 = nn.ConvTranspose2d(startOutCh4, startOutCh3, 3, 1, 1)\n",
        "        self.Conv2D_T2 = nn.ConvTranspose2d(startOutCh3, startOutCh2, 3, 1, 1)\n",
        "        self.Conv2D_T3 = nn.ConvTranspose2d(startOutCh2, startOutCh, 3, 1, 1)\n",
        "        self.Conv2D_T4 = nn.ConvTranspose2d(startOutCh, 3, 3, 1, 1)\n",
        "        \n",
        "        # Pooling & Up-Scaling Layers\n",
        "        self.Pooling_1 = nn.MaxPool2d(4, 4)\n",
        "        self.Expanding_1 = nn.UpsamplingNearest2d(scale_factor = 4)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.Sigmoid = nn.Sigmoid()      \n",
        "\n",
        "    def forward(self, x):   \n",
        "        x1 = self.Conv2D_1(x)\n",
        "        x = self.ReLU(self.Pooling_1(x1))\n",
        "        x2 = self.Conv2D_2(x)\n",
        "        x = self.ReLU(self.Pooling_1(x2))\n",
        "        x3 = self.Conv2D_3(x)\n",
        "        x = self.ReLU(self.Pooling_1(x3))\n",
        "        \n",
        "        x4 = self.Conv2D_4(x)\n",
        "        # x = self.ReLU(x4)\n",
        "        x = self.Conv2D_T1(x4)\n",
        "        x = self.ReLU(x) + self.Conv2D_T1(x4)\n",
        "        \n",
        "        x = self.Expanding_1(self.Conv2D_T2(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T2(x3)\n",
        "        x = self.Expanding_1(self.Conv2D_T3(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T3(x2)\n",
        "        x = self.Expanding_1(self.Conv2D_T4(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T4(x1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qHSsWBBmdrUF",
        "outputId": "93e5a7f8-6698-43b0-f215-db930385d303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Aug  8 11:21:51 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 536.67                 Driver Version: 536.67       CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   52C    P0              14W /  80W |      0MiB /  4096MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "model = Autoencoder()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjLbfnzIdrUG",
        "outputId": "dc789072-ee09-4a52-edb5-66727b499d53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14565376"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f__SythpGar1"
      },
      "outputs": [],
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch=None):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\"\"\"\n",
        "    if epoch is None:\n",
        "        return \"model_{0}_bs{1}_lr{2}\".format(name, batch_size, learning_rate)\n",
        "    else:\n",
        "        return \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, learning_rate, epoch)\n",
        "\n",
        "def plt_img_tensor(tensor):\n",
        "    t = torch.transpose(torch.transpose(tensor, 0, 2), 0, 1)\n",
        "    plt.imshow(t.detach().cpu())\n",
        "    plt.show()\n",
        "\n",
        "def save_img_tensor(tensor, save_dir, img_name):\n",
        "    t = torch.clamp(tensor * 255., min=0., max=255.).byte().detach().cpu()\n",
        "    torchio.write_png(t, save_dir + img_name + \".png\", compression_level=0)\n",
        "\n",
        "\n",
        "def train(model, model_dir, result_dir, train_ds, valid_ds, num_epochs=5, batch_size=1, lr=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.L1Loss() # L1 Loss is used for model updates\n",
        "    criterion_compair = nn.MSELoss() # mean square error loss for standarization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            \n",
        "            noisy, truth = data\n",
        "            \n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              noisy = noisy.cuda()\n",
        "              truth = truth.cuda()\n",
        "            #############################################\n",
        "            \n",
        "            recon = model(noisy)\n",
        "            loss = criterion(recon, truth)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        loss_oimg = criterion_compair(recon, truth)\n",
        "        print('Epoch:{}, MSE Loss:{:.4f}'.format(epoch+1, float(loss_oimg)))\n",
        "\n",
        "        for i in range(recon.size()[0]):\n",
        "            save_img_tensor(truth[i], result_dir, \"truth\" + str(epoch + 1) + \"_\" + str(i))\n",
        "            save_img_tensor(recon[i], result_dir, \"recon\" + str(epoch + 1) + \"_\" + str(i))\n",
        "            save_img_tensor(noisy[i], result_dir, \"noisy\" + str(epoch + 1) + \"_\" + str(i))\n",
        "\n",
        "        if not LOCAL:\n",
        "            plt_img_tensor(truth[0])\n",
        "            plt_img_tensor(recon[0])\n",
        "\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(model.name, batch_size, lr, epoch + 1)\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, model_path))\n",
        "\n",
        "        #outputs.append((epoch, img, recon),)\n",
        "    #return outputs\n",
        "\n",
        "#plt.imshow(np.transpose(final_recon[0][0].detach().numpy()), (1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BhBZrf-ek90l"
      },
      "outputs": [],
      "source": [
        "# Address for Datasets within the Drive\n",
        "BasePath = \"data/\" if LOCAL else \"/content/drive/MyDrive/data/\"\n",
        "BaseAnimPath = BasePath + \"baseline_data/animations/\"\n",
        "truthPaths = [BaseAnimPath + \"anim1/4096\",\n",
        "              BaseAnimPath + \"anim5/4096\"]\n",
        "noisyPaths = [BaseAnimPath + \"anim1/1\",\n",
        "              BaseAnimPath + \"anim5/1\"]\n",
        "\n",
        "def get_io_paths(noisy_dirs, truth_dirs):\n",
        "  \"\"\"\n",
        "        noisy_dirs: List of noisy image directories.\n",
        "        truth_dirs: List of truth image directories.\n",
        "  \"\"\"\n",
        "  assert len(noisy_dirs) == len(truth_dirs)\n",
        "\n",
        "  paths = []\n",
        "  for i in range(len(noisy_dirs)):\n",
        "    nfiles = [os.path.join(noisy_dirs[i], f) for f in os.listdir(noisy_dirs[i])]\n",
        "    tfiles = [os.path.join(truth_dirs[i], f) for f in os.listdir(truth_dirs[i])]\n",
        "    nfiles.sort()\n",
        "    tfiles.sort()\n",
        "    # assert len(nfiles) == len(tfiles)\n",
        "    paths.extend(zip(nfiles, tfiles))\n",
        "\n",
        "  return paths\n",
        "\n",
        "# this is the only thing that works to avoid running out of RAM in Colab\n",
        "class ImagesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, io_paths, transform=None):\n",
        "        \"\"\"\n",
        "        io_paths: list of tuples of input-output image paths.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.paths = io_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      noisy_img = torchio.read_image(self.paths[idx][0], torchio.ImageReadMode.RGB).to(torch.float)\n",
        "      truth_img = torchio.read_image(self.paths[idx][1], torchio.ImageReadMode.RGB).to(torch.float)\n",
        "      noisy_img /= 255.\n",
        "      truth_img /= 255.\n",
        "\n",
        "      if (self.transform):\n",
        "        noisy_img = self.transform(noisy_img)\n",
        "        truth_img = self.transform(truth_img)\n",
        "\n",
        "      return [noisy_img, truth_img]\n",
        "\n",
        "\n",
        "def write_crops(img, img_idx, dir):\n",
        "  crop_paths = []\n",
        "  for j in range(64):\n",
        "    crop_idx_x = j % 8\n",
        "    crop_idx_y = int(j / 8)\n",
        "    img_crop = TF.crop(img, 128 * crop_idx_y, 128 * crop_idx_x, 128, 128)\n",
        "    crop_path = dir + str(img_idx) + \"_\" + str(j) + \".png\"\n",
        "    torchio.write_png(img_crop, crop_path, compression_level=0)\n",
        "    crop_paths.append(crop_path)\n",
        "  return crop_paths\n",
        "\n",
        "\n",
        "train_split = 0.7\n",
        "valid_split = 0.15\n",
        "test_split = 0.15\n",
        "\n",
        "crop_dir = BaseAnimPath + \"crops/\"\n",
        "noisy_crop_dir = crop_dir + \"noisy/\"\n",
        "truth_crop_dir = crop_dir + \"truth/\"\n",
        "\n",
        "if REFRESH_CROPS and os.path.exists(crop_dir):\n",
        "  shutil.rmtree(crop_dir)\n",
        "\n",
        "if os.path.exists(crop_dir):\n",
        "  paths = get_io_paths([noisy_crop_dir], [truth_crop_dir])\n",
        "else:\n",
        "  paths = get_io_paths(noisyPaths, truthPaths)\n",
        "  new_paths = []\n",
        "  os.mkdir(crop_dir)\n",
        "  os.mkdir(noisy_crop_dir)\n",
        "  os.mkdir(truth_crop_dir)\n",
        "\n",
        "  for i in range(len(paths)):\n",
        "    noisy_img = torchio.read_image(paths[i][0], torchio.ImageReadMode.RGB)\n",
        "    truth_img = torchio.read_image(paths[i][1], torchio.ImageReadMode.RGB)\n",
        "    ncrop_paths = write_crops(noisy_img, i, noisy_crop_dir)\n",
        "    tcrop_paths = write_crops(truth_img, i, truth_crop_dir)\n",
        "    new_paths.extend(zip(ncrop_paths, tcrop_paths))\n",
        "\n",
        "  paths = new_paths\n",
        "  \n",
        "random.shuffle(paths)\n",
        "train_eidx = int(len(paths) * train_split)\n",
        "valid_eidx = int(len(paths) * (train_split + valid_split))\n",
        "\n",
        "#trans256 = transforms.CenterCrop(size = 256)\n",
        "train_ds = ImagesDataset(paths[:train_eidx])#, transform=trans256)\n",
        "valid_ds = ImagesDataset(paths[train_eidx:valid_eidx])#, transform=trans256)\n",
        "test_ds = ImagesDataset(paths[valid_eidx:])#, transform=trans256)\n",
        "\n",
        "BatchSize = 4\n",
        "LearningRate = 1e-3 #0.0005\n",
        "\n",
        "\n",
        "model = Autoencoder().cuda()\n",
        "model_dir = BasePath + \"nnmodel/{}/\".format(model.name)\n",
        "if not os.path.exists(model_dir):\n",
        "  os.mkdir(model_dir)\n",
        "  \n",
        "result_dir = model_dir + get_model_name(model.name, BatchSize, LearningRate) + \"_results/\"\n",
        "if not os.path.exists(result_dir):\n",
        "    os.mkdir(result_dir)\n",
        "\n",
        "fpaths = open(os.path.join(model_dir, \"paths.txt\"), \"w\")\n",
        "fpaths.write(\"split={0},{1},{2}\".format(train_split, valid_split, test_split) + \"\\n\")\n",
        "for p in paths:\n",
        "  fpaths.write(str(p) + \"\\n\")\n",
        "fpaths.close()\n",
        "\n",
        "#train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2)\n",
        "#plt.imshow(np.transpose(np.array(next(iter(train_loader))[0][0]), (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iJJXH1mdrUH"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.list_gpu_processes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oiQICbM8drUH",
        "outputId": "b17a47a5-5f37-44a0-d868-838fb3dab0c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, MSE Loss:0.0019\n",
            "Epoch:2, MSE Loss:0.0013\n",
            "Epoch:3, MSE Loss:0.0007\n",
            "Epoch:4, MSE Loss:0.0035\n",
            "Epoch:5, MSE Loss:0.0008\n",
            "Epoch:6, MSE Loss:0.0006\n",
            "Epoch:7, MSE Loss:0.0039\n"
          ]
        }
      ],
      "source": [
        "train(model, model_dir, result_dir, train_ds, valid_ds, batch_size = BatchSize, num_epochs = 250, lr = LearningRate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "544c7e72ceabb8b5baf00146d00b7c3a5bfd60d2cf41bb549c5c035fb33480da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
