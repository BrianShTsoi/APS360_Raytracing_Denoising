{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNgr3Hl5FJOa",
        "outputId": "e4ccfa6a-1e74-4f31-e051-8de0f2e03257"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%shell` not found.\n"
          ]
        }
      ],
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html \"/content/APS360Project.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2kTRL7K9DfXZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.io as torchio\n",
        "from   torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.datasets as tds\n",
        "import matplotlib.pyplot as plt\n",
        "#import pandas as pd\n",
        "import PIL\n",
        "import shutil\n",
        "\n",
        "# Mounting Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "LOCAL = True\n",
        "SHOW_RESULTS = False # !LOCAL\n",
        "# set to true when animation images have changed to rebuild\n",
        "# all the 128x128 crops. After first run, this can be set back to false\n",
        "REFRESH_CROPS = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1uRn4h2bdrUF"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    # 32 is what the paper starts with\n",
        "    def __init__(self, startOutCh = 32, depthRatio = 16 / 9):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Added Name\n",
        "        self.name = \"AEhalf_final_sod{0}_odr{1:.2f}\".format(startOutCh, depthRatio)\n",
        "        self.startOutCh = startOutCh\n",
        "        \n",
        "        # Values\n",
        "        startOutCh2 = int(startOutCh * depthRatio)\n",
        "        startOutCh3 = int(startOutCh2 * depthRatio)\n",
        "        startOutCh4 = int(startOutCh3 * depthRatio)\n",
        "        \n",
        "        # Convolution Layers\n",
        "        self.Conv2D_1 = nn.Conv2d(in_channels = 3, out_channels = startOutCh, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.Conv2D_2 = nn.Conv2d(startOutCh, startOutCh2, 3, 1, 1)\n",
        "        self.Conv2D_3 = nn.Conv2d(startOutCh2, startOutCh3, 3, 1, 1)\n",
        "        self.Conv2D_4 = nn.Conv2d(startOutCh3, startOutCh4, 3, 1, 1)\n",
        "        \n",
        "        self.Conv2D_T1 = nn.ConvTranspose2d(startOutCh4, startOutCh3, 3, 1, 1)\n",
        "        self.Conv2D_T2 = nn.ConvTranspose2d(startOutCh3, startOutCh2, 3, 1, 1)\n",
        "        self.Conv2D_T3 = nn.ConvTranspose2d(startOutCh2, startOutCh, 3, 1, 1)\n",
        "        self.Conv2D_T4 = nn.ConvTranspose2d(startOutCh, 3, 3, 1, 1)\n",
        "        \n",
        "        # Pooling & Up-Scaling Layers\n",
        "        self.Pooling_1 = nn.MaxPool2d(4, 4)\n",
        "        self.Expanding_1 = nn.UpsamplingNearest2d(scale_factor = 4)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.Sigmoid = nn.Sigmoid()      \n",
        "\n",
        "    def forward(self, x):   \n",
        "        x1 = self.Conv2D_1(x)\n",
        "        x = self.ReLU(self.Pooling_1(x1))\n",
        "        x2 = self.Conv2D_2(x)\n",
        "        x = self.ReLU(self.Pooling_1(x2))\n",
        "        x3 = self.Conv2D_3(x)\n",
        "        x = self.ReLU(self.Pooling_1(x3))\n",
        "        \n",
        "        x4 = self.Conv2D_4(x)\n",
        "        # x = self.ReLU(x4)\n",
        "        x = self.Conv2D_T1(x4)\n",
        "        x = self.ReLU(x) + self.Conv2D_T1(x4)\n",
        "        \n",
        "        x = self.Expanding_1(self.Conv2D_T2(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T2(x3)\n",
        "        x = self.Expanding_1(self.Conv2D_T3(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T3(x2)\n",
        "        x = self.Expanding_1(self.Conv2D_T4(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T4(x1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qHSsWBBmdrUF",
        "outputId": "93e5a7f8-6698-43b0-f215-db930385d303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Aug 12 05:54:01 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 536.99                 Driver Version: 536.99       CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   66C    P8               3W /  80W |   1356MiB /  4096MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A     16364      C   ...rograms\\Python\\Python311\\python.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "model = Autoencoder()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pjLbfnzIdrUG",
        "outputId": "dc789072-ee09-4a52-edb5-66727b499d53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "125144064"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f__SythpGar1"
      },
      "outputs": [],
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch=None):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\"\"\"\n",
        "    if epoch is None:\n",
        "        return \"model_{0}_bs{1}_lr{2}\".format(name, batch_size, learning_rate)\n",
        "    else:\n",
        "        return \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, learning_rate, epoch)\n",
        "\n",
        "def plt_img_tensor(tensor):\n",
        "    t = torch.transpose(torch.transpose(tensor, 0, 2), 0, 1)\n",
        "    plt.imshow(t.detach().cpu())\n",
        "    plt.show()\n",
        "\n",
        "def save_img_tensor(tensor, save_dir, img_name):\n",
        "    t = torch.clamp(tensor * 255., min=0., max=255.).byte().detach().cpu()\n",
        "    torchio.write_png(t, save_dir + img_name + \".png\", compression_level=0)\n",
        "\n",
        "\n",
        "def train(model, model_dir, result_dir, train_ds, valid_ds, num_epochs=5, batch_size=1, lr=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.L1Loss() # L1 Loss is used for model updates\n",
        "    criterion_compair = nn.MSELoss() # mean square error loss for standarization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss = 0.\n",
        "        for data in train_loader:\n",
        "            noisy, truth = data            \n",
        "            recon = model(noisy)\n",
        "            loss = criterion(recon, truth)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            avg_loss += float(loss)\n",
        "            \n",
        "        #loss_oimg = criterion_compair(recon, truth)\n",
        "        avg_loss /= len(train_loader)\n",
        "        print('Epoch:{}, MSE Loss:{:.4f}'.format(epoch+1, avg_loss))\n",
        "        \n",
        "        for i in range(recon.size()[0]):\n",
        "            save_img_tensor(truth[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_truth\")\n",
        "            save_img_tensor(recon[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_recon\")\n",
        "            save_img_tensor(noisy[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_noisy\")\n",
        "\n",
        "        if SHOW_RESULTS:\n",
        "            plt_img_tensor(truth[0])\n",
        "            plt_img_tensor(recon[0])\n",
        "\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(model.name, batch_size, lr, epoch + 1)\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, model_path))\n",
        "\n",
        "        #outputs.append((epoch, img, recon),)\n",
        "    #return outputs\n",
        "\n",
        "#plt.imshow(np.transpose(final_recon[0][0].detach().numpy()), (1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BhBZrf-ek90l"
      },
      "outputs": [],
      "source": [
        "# Address for Datasets within the Drive\n",
        "BasePath = \"data/\" if LOCAL else \"/content/drive/MyDrive/data/\"\n",
        "BaseAnimPath = BasePath + \"baseline_data/animations/\"\n",
        "truthPaths = [BaseAnimPath + \"anim1/4096\",\n",
        "              BaseAnimPath + \"anim2/4096\",\n",
        "              #BaseAnimPath + \"anim3/4096\",\n",
        "              BaseAnimPath + \"anim5/4096\"]\n",
        "noisyPaths = [BaseAnimPath + \"anim1/1\",\n",
        "              BaseAnimPath + \"anim2/1\",\n",
        "              #BaseAnimPath + \"anim3/1\",\n",
        "              BaseAnimPath + \"anim5/1\"]\n",
        "\n",
        "def get_io_paths(noisy_dirs, truth_dirs):\n",
        "  \"\"\"\n",
        "        noisy_dirs: List of noisy image directories.\n",
        "        truth_dirs: List of truth image directories.\n",
        "  \"\"\"\n",
        "  assert len(noisy_dirs) == len(truth_dirs)\n",
        "\n",
        "  paths = []\n",
        "  for i in range(len(noisy_dirs)):\n",
        "    nfiles = [os.path.join(noisy_dirs[i], f) for f in os.listdir(noisy_dirs[i])]\n",
        "    tfiles = [os.path.join(truth_dirs[i], f) for f in os.listdir(truth_dirs[i])]\n",
        "    nfiles.sort()\n",
        "    tfiles.sort()\n",
        "    # assert len(nfiles) == len(tfiles)\n",
        "    paths.extend(zip(nfiles, tfiles))\n",
        "\n",
        "  return paths\n",
        "\n",
        "# this is the only thing that works to avoid running out of RAM in Colab\n",
        "class ImagesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, io_paths, transform=None):\n",
        "        \"\"\"\n",
        "        io_paths: list of tuples of input-output image paths.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.paths = io_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      noisy_img = torchio.read_image(self.paths[idx][0], torchio.ImageReadMode.RGB).to(torch.float)\n",
        "      truth_img = torchio.read_image(self.paths[idx][1], torchio.ImageReadMode.RGB).to(torch.float)\n",
        "\n",
        "      if use_cuda and torch.cuda.is_available():\n",
        "        noisy_img = noisy_img.cuda()\n",
        "        truth_img = truth_img.cuda()\n",
        "         \n",
        "      noisy_img /= 255.\n",
        "      truth_img /= 255.\n",
        "\n",
        "      if (self.transform):\n",
        "        noisy_img = self.transform(noisy_img)\n",
        "        truth_img = self.transform(truth_img)\n",
        "\n",
        "      return [noisy_img, truth_img]\n",
        "\n",
        "\n",
        "def write_crops(img, img_idx, dir):\n",
        "  crop_paths = []\n",
        "  for j in range(64):\n",
        "    crop_idx_x = j % 8\n",
        "    crop_idx_y = int(j / 8)\n",
        "    img_crop = TF.crop(img, 128 * crop_idx_y, 128 * crop_idx_x, 128, 128)\n",
        "    crop_path = dir + str(img_idx) + \"_\" + str(j) + \".png\"\n",
        "    torchio.write_png(img_crop, crop_path, compression_level=0)\n",
        "    crop_paths.append(crop_path)\n",
        "  return crop_paths\n",
        "\n",
        "\n",
        "train_split = 0.7\n",
        "valid_split = 0.15\n",
        "test_split = 0.15\n",
        "\n",
        "crop_dir = BaseAnimPath + \"crops/\"\n",
        "noisy_crop_dir = crop_dir + \"noisy/\"\n",
        "truth_crop_dir = crop_dir + \"truth/\"\n",
        "\n",
        "if REFRESH_CROPS and os.path.exists(crop_dir):\n",
        "  shutil.rmtree(crop_dir)\n",
        "\n",
        "if os.path.exists(crop_dir):\n",
        "  paths = get_io_paths([noisy_crop_dir], [truth_crop_dir])\n",
        "else:\n",
        "  paths = get_io_paths(noisyPaths, truthPaths)\n",
        "  new_paths = []\n",
        "  os.mkdir(crop_dir)\n",
        "  os.mkdir(noisy_crop_dir)\n",
        "  os.mkdir(truth_crop_dir)\n",
        "\n",
        "  for i in range(len(paths)):\n",
        "    noisy_img = torchio.read_image(paths[i][0], torchio.ImageReadMode.RGB)\n",
        "    truth_img = torchio.read_image(paths[i][1], torchio.ImageReadMode.RGB)\n",
        "    ncrop_paths = write_crops(noisy_img, i, noisy_crop_dir)\n",
        "    tcrop_paths = write_crops(truth_img, i, truth_crop_dir)\n",
        "    new_paths.extend(zip(ncrop_paths, tcrop_paths))\n",
        "\n",
        "  paths = new_paths\n",
        "  \n",
        "random.shuffle(paths)\n",
        "train_eidx = int(len(paths) * train_split)\n",
        "valid_eidx = int(len(paths) * (train_split + valid_split))\n",
        "\n",
        "#trans256 = transforms.CenterCrop(size = 256)\n",
        "train_ds = ImagesDataset(paths[:train_eidx])#, transform=trans256)\n",
        "valid_ds = ImagesDataset(paths[train_eidx:valid_eidx])#, transform=trans256)\n",
        "test_ds = ImagesDataset(paths[valid_eidx:])#, transform=trans256)\n",
        "\n",
        "BatchSize = 16\n",
        "LearningRate = 5e-4\n",
        "\n",
        "\n",
        "model = Autoencoder().cuda()\n",
        "model_dir = BasePath + \"nnmodel/{}/\".format(model.name)\n",
        "if not os.path.exists(model_dir):\n",
        "  os.mkdir(model_dir)\n",
        "  \n",
        "result_dir = model_dir + get_model_name(model.name, BatchSize, LearningRate) + \"_results/\"\n",
        "if not os.path.exists(result_dir):\n",
        "    os.mkdir(result_dir)\n",
        "\n",
        "fpaths = open(os.path.join(model_dir, \"paths.txt\"), \"w\")\n",
        "fpaths.write(\"split={0},{1},{2}\".format(train_split, valid_split, test_split) + \"\\n\")\n",
        "for p in paths:\n",
        "  fpaths.write(str(p) + \"\\n\")\n",
        "fpaths.close()\n",
        "\n",
        "#train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2)\n",
        "#plt.imshow(np.transpose(np.array(next(iter(train_loader))[0][0]), (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"data/nnmodel/AEhalfmdatar3_sod32_odr1.78/model_AEhalfmdatar3_sod32_odr1.78_bs16_lr0.0005_epoch75\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_iJJXH1mdrUH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 1024, 1024])\n",
            "MSE loss recon 0.012149984948337078\n",
            "MSE loss guassian blur k5 0.0750330463051796\n"
          ]
        }
      ],
      "source": [
        "demo_path = r\"test2\\breakfast32.png\"\n",
        "gt_path = r\"test2\\breakfast8192.png\"\n",
        "gblur_path = r\"test2\\breakfast32_guassianblur.png\"\n",
        "\n",
        "demo_ds = ImagesDataset([(demo_path, gt_path), (gblur_path, gt_path),])\n",
        "#save_img_tensor(next(iter(demo_ds))[0], \"E:\\\\Maya\\\\UofT\\\\8th sem\\\\aps360\\\\project\\\\APS360_raytracing_denoising\\\\localver\\\\\", \"demo1cropped.png\")\n",
        "\n",
        "demo_dl = torch.utils.data.DataLoader(demo_ds, batch_size=1)\n",
        "\n",
        "demo_dat = next(iter(demo_dl))\n",
        "demo_noisy, demo_gt = demo_dat\n",
        "demo_dat2 = next(iter(demo_dl))\n",
        "demo_gblur, _ = demo_dat2\n",
        "\n",
        "demo_recon = model(demo_noisy)\n",
        "\n",
        "cmp = nn.MSELoss()\n",
        "print(\"MSE loss recon\", float(cmp(demo_recon, demo_gt)))\n",
        "print(\"MSE loss guassian blur k5\", float(cmp(demo_gblur, demo_gt)))\n",
        "\n",
        "save_img_tensor(demo_recon[0], \"test2\", \"breakfast_recon\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg MSE of scene: 0.0013735576951876282\n"
          ]
        }
      ],
      "source": [
        "mse_cmp_ds = ImagesDataset(get_io_paths([BaseAnimPath + \"anim3/1\"], [BaseAnimPath + \"anim3/4096\"]))\n",
        "cmp_loss_fn = nn.MSELoss()\n",
        "\n",
        "avg_mse = 0.\n",
        "for data in iter(mse_cmp_ds):\n",
        "    noisy, truth = data\n",
        "    avg_mse += float(cmp_loss_fn(noisy, truth))\n",
        "\n",
        "avg_mse /= len(mse_cmp_ds)\n",
        "print(\"Avg MSE of scene:\", avg_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oiQICbM8drUH",
        "outputId": "b17a47a5-5f37-44a0-d868-838fb3dab0c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, MSE Loss:0.0455\n",
            "Epoch:2, MSE Loss:0.0400\n",
            "Epoch:3, MSE Loss:0.0396\n",
            "Epoch:4, MSE Loss:0.0393\n",
            "Epoch:5, MSE Loss:0.0394\n",
            "Epoch:6, MSE Loss:0.0391\n",
            "Epoch:7, MSE Loss:0.0390\n",
            "Epoch:8, MSE Loss:0.0390\n",
            "Epoch:9, MSE Loss:0.0389\n",
            "Epoch:10, MSE Loss:0.0389\n",
            "Epoch:11, MSE Loss:0.0388\n",
            "Epoch:12, MSE Loss:0.0388\n",
            "Epoch:13, MSE Loss:0.0388\n",
            "Epoch:14, MSE Loss:0.0387\n",
            "Epoch:15, MSE Loss:0.0387\n",
            "Epoch:16, MSE Loss:0.0387\n",
            "Epoch:17, MSE Loss:0.0387\n",
            "Epoch:18, MSE Loss:0.0388\n",
            "Epoch:19, MSE Loss:0.0386\n",
            "Epoch:20, MSE Loss:0.0386\n",
            "Epoch:21, MSE Loss:0.0386\n",
            "Epoch:22, MSE Loss:0.0386\n",
            "Epoch:23, MSE Loss:0.0386\n",
            "Epoch:24, MSE Loss:0.0386\n",
            "Epoch:25, MSE Loss:0.0255\n",
            "Epoch:26, MSE Loss:0.0197\n",
            "Epoch:27, MSE Loss:0.0195\n",
            "Epoch:28, MSE Loss:0.0193\n",
            "Epoch:29, MSE Loss:0.0190\n",
            "Epoch:30, MSE Loss:0.0190\n",
            "Epoch:31, MSE Loss:0.0190\n",
            "Epoch:32, MSE Loss:0.0190\n",
            "Epoch:33, MSE Loss:0.0188\n",
            "Epoch:34, MSE Loss:0.0188\n",
            "Epoch:35, MSE Loss:0.0188\n",
            "Epoch:36, MSE Loss:0.0188\n",
            "Epoch:37, MSE Loss:0.0187\n",
            "Epoch:38, MSE Loss:0.0188\n",
            "Epoch:39, MSE Loss:0.0187\n",
            "Epoch:40, MSE Loss:0.0187\n",
            "Epoch:41, MSE Loss:0.0187\n",
            "Epoch:42, MSE Loss:0.0187\n",
            "Epoch:43, MSE Loss:0.0187\n",
            "Epoch:44, MSE Loss:0.0186\n",
            "Epoch:45, MSE Loss:0.0187\n",
            "Epoch:46, MSE Loss:0.0187\n",
            "Epoch:47, MSE Loss:0.0187\n",
            "Epoch:48, MSE Loss:0.0186\n",
            "Epoch:49, MSE Loss:0.0186\n",
            "Epoch:50, MSE Loss:0.0187\n",
            "Epoch:51, MSE Loss:0.0187\n",
            "Epoch:52, MSE Loss:0.0187\n",
            "Epoch:53, MSE Loss:0.0186\n",
            "Epoch:54, MSE Loss:0.0185\n",
            "Epoch:55, MSE Loss:0.0186\n",
            "Epoch:56, MSE Loss:0.0185\n",
            "Epoch:57, MSE Loss:0.0185\n",
            "Epoch:58, MSE Loss:0.0185\n",
            "Epoch:59, MSE Loss:0.0185\n",
            "Epoch:60, MSE Loss:0.0184\n",
            "Epoch:61, MSE Loss:0.0184\n",
            "Epoch:62, MSE Loss:0.0182\n",
            "Epoch:63, MSE Loss:0.0182\n",
            "Epoch:64, MSE Loss:0.0180\n",
            "Epoch:65, MSE Loss:0.0179\n",
            "Epoch:66, MSE Loss:0.0179\n",
            "Epoch:67, MSE Loss:0.0179\n",
            "Epoch:68, MSE Loss:0.0179\n",
            "Epoch:69, MSE Loss:0.0178\n",
            "Epoch:70, MSE Loss:0.0179\n",
            "Epoch:71, MSE Loss:0.0178\n",
            "Epoch:72, MSE Loss:0.0178\n",
            "Epoch:73, MSE Loss:0.0178\n",
            "Epoch:74, MSE Loss:0.0178\n",
            "Epoch:75, MSE Loss:0.0178\n",
            "Epoch:76, MSE Loss:0.0178\n",
            "Epoch:77, MSE Loss:0.0178\n",
            "Epoch:78, MSE Loss:0.0178\n",
            "Epoch:79, MSE Loss:0.0178\n",
            "Epoch:80, MSE Loss:0.0178\n",
            "Epoch:81, MSE Loss:0.0177\n",
            "Epoch:82, MSE Loss:0.0177\n",
            "Epoch:83, MSE Loss:0.0178\n",
            "Epoch:84, MSE Loss:0.0177\n",
            "Epoch:85, MSE Loss:0.0177\n",
            "Epoch:86, MSE Loss:0.0178\n",
            "Epoch:87, MSE Loss:0.0177\n",
            "Epoch:88, MSE Loss:0.0177\n",
            "Epoch:89, MSE Loss:0.0177\n",
            "Epoch:90, MSE Loss:0.0177\n",
            "Epoch:91, MSE Loss:0.0177\n",
            "Epoch:92, MSE Loss:0.0177\n",
            "Epoch:93, MSE Loss:0.0177\n",
            "Epoch:94, MSE Loss:0.0177\n",
            "Epoch:95, MSE Loss:0.0177\n",
            "Epoch:96, MSE Loss:0.0177\n",
            "Epoch:97, MSE Loss:0.0177\n",
            "Epoch:98, MSE Loss:0.0177\n",
            "Epoch:99, MSE Loss:0.0177\n",
            "Epoch:100, MSE Loss:0.0177\n",
            "Epoch:101, MSE Loss:0.0177\n",
            "Epoch:102, MSE Loss:0.0177\n",
            "Epoch:103, MSE Loss:0.0177\n",
            "Epoch:104, MSE Loss:0.0177\n",
            "Epoch:105, MSE Loss:0.0177\n",
            "Epoch:106, MSE Loss:0.0177\n",
            "Epoch:107, MSE Loss:0.0177\n",
            "Epoch:108, MSE Loss:0.0177\n",
            "Epoch:109, MSE Loss:0.0177\n",
            "Epoch:110, MSE Loss:0.0176\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, model_dir, result_dir, train_ds, valid_ds, batch_size \u001b[39m=\u001b[39;49m BatchSize, num_epochs \u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m, lr \u001b[39m=\u001b[39;49m LearningRate)\n",
            "Cell \u001b[1;32mIn[14], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, model_dir, result_dir, train_ds, valid_ds, num_epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 36\u001b[0m     avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(loss)\n\u001b[0;32m     38\u001b[0m \u001b[39m#loss_oimg = criterion_compair(recon, truth)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m avg_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(model, model_dir, result_dir, train_ds, valid_ds, batch_size = BatchSize, num_epochs = 500, lr = LearningRate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "544c7e72ceabb8b5baf00146d00b7c3a5bfd60d2cf41bb549c5c035fb33480da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
