{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNgr3Hl5FJOa",
        "outputId": "e4ccfa6a-1e74-4f31-e051-8de0f2e03257"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%shell` not found.\n"
          ]
        }
      ],
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html \"/content/APS360Project.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2kTRL7K9DfXZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.io as torchio\n",
        "from   torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.datasets as tds\n",
        "import matplotlib.pyplot as plt\n",
        "#import pandas as pd\n",
        "import PIL\n",
        "import shutil\n",
        "\n",
        "# Mounting Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "LOCAL = True\n",
        "SHOW_RESULTS = False # !LOCAL\n",
        "# set to true when animation images have changed to rebuild\n",
        "# all the 128x128 crops. After first run, this can be set back to false\n",
        "REFRESH_CROPS = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "1uRn4h2bdrUF"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    # 32 is what the paper starts with\n",
        "    def __init__(self, startOutCh = 32, depthRatio = 16 / 9):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Added Name\n",
        "        self.name = \"AEhalfmdata_sod{0}_odr{1:.2f}\".format(startOutCh, depthRatio)\n",
        "        self.startOutCh = startOutCh\n",
        "        \n",
        "        # Values\n",
        "        startOutCh2 = int(startOutCh * depthRatio)\n",
        "        startOutCh3 = int(startOutCh2 * depthRatio)\n",
        "        startOutCh4 = int(startOutCh3 * depthRatio)\n",
        "        \n",
        "        # Convolution Layers\n",
        "        self.Conv2D_1 = nn.Conv2d(in_channels = 3, out_channels = startOutCh, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.Conv2D_2 = nn.Conv2d(startOutCh, startOutCh2, 3, 1, 1)\n",
        "        self.Conv2D_3 = nn.Conv2d(startOutCh2, startOutCh3, 3, 1, 1)\n",
        "        self.Conv2D_4 = nn.Conv2d(startOutCh3, startOutCh4, 3, 1, 1)\n",
        "        \n",
        "        self.Conv2D_T1 = nn.ConvTranspose2d(startOutCh4, startOutCh3, 3, 1, 1)\n",
        "        self.Conv2D_T2 = nn.ConvTranspose2d(startOutCh3, startOutCh2, 3, 1, 1)\n",
        "        self.Conv2D_T3 = nn.ConvTranspose2d(startOutCh2, startOutCh, 3, 1, 1)\n",
        "        self.Conv2D_T4 = nn.ConvTranspose2d(startOutCh, 3, 3, 1, 1)\n",
        "        \n",
        "        # Pooling & Up-Scaling Layers\n",
        "        self.Pooling_1 = nn.MaxPool2d(4, 4)\n",
        "        self.Expanding_1 = nn.UpsamplingNearest2d(scale_factor = 4)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.Sigmoid = nn.Sigmoid()      \n",
        "\n",
        "    def forward(self, x):   \n",
        "        x1 = self.Conv2D_1(x)\n",
        "        x = self.ReLU(self.Pooling_1(x1))\n",
        "        x2 = self.Conv2D_2(x)\n",
        "        x = self.ReLU(self.Pooling_1(x2))\n",
        "        x3 = self.Conv2D_3(x)\n",
        "        x = self.ReLU(self.Pooling_1(x3))\n",
        "        \n",
        "        x4 = self.Conv2D_4(x)\n",
        "        # x = self.ReLU(x4)\n",
        "        x = self.Conv2D_T1(x4)\n",
        "        x = self.ReLU(x) + self.Conv2D_T1(x4)\n",
        "        \n",
        "        x = self.Expanding_1(self.Conv2D_T2(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T2(x3)\n",
        "        x = self.Expanding_1(self.Conv2D_T3(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T3(x2)\n",
        "        x = self.Expanding_1(self.Conv2D_T4(x))\n",
        "        x = self.ReLU(x) + self.Conv2D_T4(x1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qHSsWBBmdrUF",
        "outputId": "93e5a7f8-6698-43b0-f215-db930385d303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Aug  8 18:30:36 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 536.67                 Driver Version: 536.67       CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   50C    P8               6W /  35W |      0MiB /  4096MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      2620      C   ...rograms\\Python\\Python311\\python.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "model = Autoencoder()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "pjLbfnzIdrUG",
        "outputId": "dc789072-ee09-4a52-edb5-66727b499d53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2585798656"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "f__SythpGar1"
      },
      "outputs": [],
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch=None):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\"\"\"\n",
        "    if epoch is None:\n",
        "        return \"model_{0}_bs{1}_lr{2}\".format(name, batch_size, learning_rate)\n",
        "    else:\n",
        "        return \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, learning_rate, epoch)\n",
        "\n",
        "def plt_img_tensor(tensor):\n",
        "    t = torch.transpose(torch.transpose(tensor, 0, 2), 0, 1)\n",
        "    plt.imshow(t.detach().cpu())\n",
        "    plt.show()\n",
        "\n",
        "def save_img_tensor(tensor, save_dir, img_name):\n",
        "    t = torch.clamp(tensor * 255., min=0., max=255.).byte().detach().cpu()\n",
        "    torchio.write_png(t, save_dir + img_name + \".png\", compression_level=0)\n",
        "\n",
        "\n",
        "def train(model, model_dir, result_dir, train_ds, valid_ds, num_epochs=5, batch_size=1, lr=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.L1Loss() # L1 Loss is used for model updates\n",
        "    criterion_compair = nn.MSELoss() # mean square error loss for standarization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            \n",
        "            noisy, truth = data\n",
        "            \n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              noisy = noisy.cuda()\n",
        "              truth = truth.cuda()\n",
        "            #############################################\n",
        "            \n",
        "            recon = model(noisy)\n",
        "            loss = criterion(recon, truth)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        loss_oimg = criterion_compair(recon, truth)\n",
        "        print('Epoch:{}, MSE Loss:{:.4f}'.format(epoch+1, float(loss_oimg)))\n",
        "\n",
        "        for i in range(recon.size()[0]):\n",
        "            save_img_tensor(truth[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_truth\")\n",
        "            save_img_tensor(recon[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_recon\")\n",
        "            save_img_tensor(noisy[i], result_dir, str(epoch + 1) + \"_\" + str(i) + \"_noisy\")\n",
        "\n",
        "        if SHOW_RESULTS:\n",
        "            plt_img_tensor(truth[0])\n",
        "            plt_img_tensor(recon[0])\n",
        "\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(model.name, batch_size, lr, epoch + 1)\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, model_path))\n",
        "\n",
        "        #outputs.append((epoch, img, recon),)\n",
        "    #return outputs\n",
        "\n",
        "#plt.imshow(np.transpose(final_recon[0][0].detach().numpy()), (1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "BhBZrf-ek90l"
      },
      "outputs": [],
      "source": [
        "# Address for Datasets within the Drive\n",
        "BasePath = \"data/\" if LOCAL else \"/content/drive/MyDrive/data/\"\n",
        "BaseAnimPath = BasePath + \"baseline_data/animations/\"\n",
        "truthPaths = [BaseAnimPath + \"anim1/4096\",\n",
        "              BaseAnimPath + \"anim2/4096\",\n",
        "              BaseAnimPath + \"anim5/4096\"]\n",
        "noisyPaths = [BaseAnimPath + \"anim1/1\",\n",
        "              BaseAnimPath + \"anim2/1\",\n",
        "              BaseAnimPath + \"anim5/1\"]\n",
        "\n",
        "def get_io_paths(noisy_dirs, truth_dirs):\n",
        "  \"\"\"\n",
        "        noisy_dirs: List of noisy image directories.\n",
        "        truth_dirs: List of truth image directories.\n",
        "  \"\"\"\n",
        "  assert len(noisy_dirs) == len(truth_dirs)\n",
        "\n",
        "  paths = []\n",
        "  for i in range(len(noisy_dirs)):\n",
        "    nfiles = [os.path.join(noisy_dirs[i], f) for f in os.listdir(noisy_dirs[i])]\n",
        "    tfiles = [os.path.join(truth_dirs[i], f) for f in os.listdir(truth_dirs[i])]\n",
        "    nfiles.sort()\n",
        "    tfiles.sort()\n",
        "    # assert len(nfiles) == len(tfiles)\n",
        "    paths.extend(zip(nfiles, tfiles))\n",
        "\n",
        "  return paths\n",
        "\n",
        "# this is the only thing that works to avoid running out of RAM in Colab\n",
        "class ImagesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, io_paths, transform=None):\n",
        "        \"\"\"\n",
        "        io_paths: list of tuples of input-output image paths.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.paths = io_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      noisy_img = torchio.read_image(self.paths[idx][0], torchio.ImageReadMode.RGB).to(torch.float)\n",
        "      truth_img = torchio.read_image(self.paths[idx][1], torchio.ImageReadMode.RGB).to(torch.float)\n",
        "      noisy_img /= 255.\n",
        "      truth_img /= 255.\n",
        "\n",
        "      if (self.transform):\n",
        "        noisy_img = self.transform(noisy_img)\n",
        "        truth_img = self.transform(truth_img)\n",
        "\n",
        "      return [noisy_img, truth_img]\n",
        "\n",
        "\n",
        "def write_crops(img, img_idx, dir):\n",
        "  crop_paths = []\n",
        "  for j in range(64):\n",
        "    crop_idx_x = j % 8\n",
        "    crop_idx_y = int(j / 8)\n",
        "    img_crop = TF.crop(img, 128 * crop_idx_y, 128 * crop_idx_x, 128, 128)\n",
        "    crop_path = dir + str(img_idx) + \"_\" + str(j) + \".png\"\n",
        "    torchio.write_png(img_crop, crop_path, compression_level=0)\n",
        "    crop_paths.append(crop_path)\n",
        "  return crop_paths\n",
        "\n",
        "\n",
        "train_split = 0.7\n",
        "valid_split = 0.15\n",
        "test_split = 0.15\n",
        "\n",
        "crop_dir = BaseAnimPath + \"crops/\"\n",
        "noisy_crop_dir = crop_dir + \"noisy/\"\n",
        "truth_crop_dir = crop_dir + \"truth/\"\n",
        "\n",
        "if REFRESH_CROPS and os.path.exists(crop_dir):\n",
        "  shutil.rmtree(crop_dir)\n",
        "\n",
        "if os.path.exists(crop_dir):\n",
        "  paths = get_io_paths([noisy_crop_dir], [truth_crop_dir])\n",
        "else:\n",
        "  paths = get_io_paths(noisyPaths, truthPaths)\n",
        "  new_paths = []\n",
        "  os.mkdir(crop_dir)\n",
        "  os.mkdir(noisy_crop_dir)\n",
        "  os.mkdir(truth_crop_dir)\n",
        "\n",
        "  for i in range(len(paths)):\n",
        "    noisy_img = torchio.read_image(paths[i][0], torchio.ImageReadMode.RGB)\n",
        "    truth_img = torchio.read_image(paths[i][1], torchio.ImageReadMode.RGB)\n",
        "    ncrop_paths = write_crops(noisy_img, i, noisy_crop_dir)\n",
        "    tcrop_paths = write_crops(truth_img, i, truth_crop_dir)\n",
        "    new_paths.extend(zip(ncrop_paths, tcrop_paths))\n",
        "\n",
        "  paths = new_paths\n",
        "  \n",
        "random.shuffle(paths)\n",
        "train_eidx = int(len(paths) * train_split)\n",
        "valid_eidx = int(len(paths) * (train_split + valid_split))\n",
        "\n",
        "#trans256 = transforms.CenterCrop(size = 256)\n",
        "train_ds = ImagesDataset(paths[:train_eidx])#, transform=trans256)\n",
        "valid_ds = ImagesDataset(paths[train_eidx:valid_eidx])#, transform=trans256)\n",
        "test_ds = ImagesDataset(paths[valid_eidx:])#, transform=trans256)\n",
        "\n",
        "BatchSize = 16\n",
        "LearningRate = 5e-4\n",
        "\n",
        "\n",
        "model = Autoencoder().cuda()\n",
        "model_dir = BasePath + \"nnmodel/{}/\".format(model.name)\n",
        "if not os.path.exists(model_dir):\n",
        "  os.mkdir(model_dir)\n",
        "  \n",
        "result_dir = model_dir + get_model_name(model.name, BatchSize, LearningRate) + \"_results/\"\n",
        "if not os.path.exists(result_dir):\n",
        "    os.mkdir(result_dir)\n",
        "\n",
        "fpaths = open(os.path.join(model_dir, \"paths.txt\"), \"w\")\n",
        "fpaths.write(\"split={0},{1},{2}\".format(train_split, valid_split, test_split) + \"\\n\")\n",
        "for p in paths:\n",
        "  fpaths.write(str(p) + \"\\n\")\n",
        "fpaths.close()\n",
        "\n",
        "#train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2)\n",
        "#plt.imshow(np.transpose(np.array(next(iter(train_loader))[0][0]), (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"data/nnmodel/AEhalfmdata_sod32_odr1.78/model_AEhalfmdata_sod32_odr1.78_bs16_lr0.0005_epoch62\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_iJJXH1mdrUH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 1024, 1024])\n"
          ]
        }
      ],
      "source": [
        "demo_path = r\"E:\\Maya\\UofT\\8th sem\\aps360\\project\\APS360_raytracing_denoising\\localver\\data\\baseline_data\\animations\\anim3\\1\\0073.png\"\n",
        "demo_ds = ImagesDataset([(demo_path, demo_path)])\n",
        "#save_img_tensor(next(iter(demo_ds))[0], \"E:\\\\Maya\\\\UofT\\\\8th sem\\\\aps360\\\\project\\\\APS360_raytracing_denoising\\\\localver\\\\\", \"demo1cropped.png\")\n",
        "\n",
        "demo_dl = torch.utils.data.DataLoader(demo_ds, batch_size=1)\n",
        "\n",
        "demo_noisy = next(iter(demo_dl))[0].cuda()\n",
        "print(demo_noisy.size())\n",
        "demo_recon = model(demo_noisy)\n",
        "\n",
        "    # plt.imshow(np.transpose(demo_noisy.detach().cpu().numpy()[0], (1, 2, 0)))\n",
        "    # plt.show()\n",
        "    # plt.imshow(np.transpose(demo_recon.detach().cpu().numpy()[0], (1, 2, 0)))\n",
        "    # plt.show()\n",
        "\n",
        "save_img_tensor(demo_recon[0], \"E:\\\\Maya\\\\UofT\\\\8th sem\\\\aps360\\\\project\\\\APS360_raytracing_denoising\\\\localver\\\\test\\\\\", \"anim3_0073\")\n",
        "    #save_img_tensor(demo_noisy[0], \"E:\\\\Maya\\\\UofT\\\\8th sem\\\\aps360\\\\project\\\\APS360_raytracing_denoising\\\\localver\\\\test\\\\\", \"noisy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "oiQICbM8drUH",
        "outputId": "b17a47a5-5f37-44a0-d868-838fb3dab0c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, MSE Loss:0.0217\n",
            "Epoch:2, MSE Loss:0.0205\n",
            "Epoch:3, MSE Loss:0.0162\n",
            "Epoch:4, MSE Loss:0.0022\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, model_dir, result_dir, train_ds, valid_ds, batch_size \u001b[39m=\u001b[39;49m BatchSize, num_epochs \u001b[39m=\u001b[39;49m \u001b[39m250\u001b[39;49m, lr \u001b[39m=\u001b[39;49m LearningRate)\n",
            "Cell \u001b[1;32mIn[33], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, model_dir, result_dir, train_ds, valid_ds, num_epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m#outputs = []\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 28\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     30\u001b[0m         noisy, truth \u001b[39m=\u001b[39m data\n\u001b[0;32m     32\u001b[0m         \u001b[39m#############################################\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         \u001b[39m#To Enable GPU Usage\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(model, model_dir, result_dir, train_ds, valid_ds, batch_size = BatchSize, num_epochs = 250, lr = LearningRate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "544c7e72ceabb8b5baf00146d00b7c3a5bfd60d2cf41bb549c5c035fb33480da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
